#!/usr/bin/env python3
"""
Codebase Health Scanner

Scans the repository and generates web/src/data/codebaseHealthData.ts
with file statistics and refactor suggestions.

Usage:
    python scripts/generate_codebase_health.py
"""

import os
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Set
from dataclasses import dataclass, field
from collections import defaultdict

# Repository root (parent of scripts/)
REPO_ROOT = Path(__file__).parent.parent
OUTPUT_FILE = REPO_ROOT / "web" / "src" / "data" / "codebaseHealthData.ts"

# Directories to exclude
EXCLUDE_DIRS = {
    'node_modules', '.venv', '__pycache__', '.git', '.pytest_cache',
    'dist', 'build', '.next', 'coverage', '.mypy_cache', 'eggs', '*.egg-info'
}

# Specific files to exclude (generated, lock files, etc.)
EXCLUDE_FILES = {
    'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml', 'poetry.lock',
    'Pipfile.lock', '.env', '.env.local', '.env.production',
    'codebaseHealthData.ts',  # Auto-generated by this script
}

# File patterns that are intentionally large data files (don't recommend splitting)
# These contain data definitions, handler registries, or constants that belong together
DATA_FILE_PATTERNS = {
    '_data.py', '_data.ts', '_definitions.py', '_handlers.py',
    'Data.ts', 'Data.tsx',  # Frontend data files
    'constants.py', 'constants.ts',
}

# File extensions to include
INCLUDE_EXTENSIONS = {
    '.py', '.ts', '.tsx', '.js', '.jsx', '.css', '.scss', '.md', '.json', '.yaml', '.yml'
}

# Known schema/framework base classes that indicate related inheritance patterns
SCHEMA_BASES = {
    # Pydantic
    'BaseModel', 'BaseSettings', 'GenericModel',
    # SQLAlchemy
    'Base', 'DeclarativeBase', 'AbstractConcreteBase',
    # Django
    'Model', 'View', 'Form', 'ModelForm', 'Serializer', 'ModelSerializer',
    # FastAPI/Starlette
    'HTTPException',
    # Generic patterns
    'ABC', 'Protocol',
}

# Size thresholds
SIZE_THRESHOLDS = {
    'small': (0, 99),
    'medium': (100, 299),
    'large': (300, 499),
    'xlarge': (500, float('inf'))
}


@dataclass
class FileStats:
    """Statistics for a single file."""
    path: str
    loc: int
    nesting_depth: int
    file_type: str
    area: str
    size_category: str


@dataclass
class RefactorItem:
    """A suggested refactoring task."""
    id: str
    title: str
    description: str
    priority: str
    status: str = 'pending'
    category: List[str] = field(default_factory=list)
    effort: str = 'medium'
    affected_files: List[str] = field(default_factory=list)
    details: List[str] = field(default_factory=list)
    automated_reason: str = ''
    technique: str = ''  # Specific refactoring approach


def pluralize(count: int, singular: str, plural: str = None) -> str:
    """Return singular or plural form based on count."""
    if plural is None:
        plural = singular + 's'
    return singular if count == 1 else plural


def find_semantic_groups(class_names: List[str]) -> Dict[str, List[str]]:
    """Find classes that are semantically related by shared name patterns.

    Examples:
        - Ghost, GhostPath, GhostManager -> grouped by 'Ghost' prefix
        - PlayerData, EnemyData, ItemData -> grouped by 'Data' suffix
        - Room, BSPNode, Dungeon -> not grouped (no pattern)
    """
    if len(class_names) < 2:
        return {}

    groups: Dict[str, List[str]] = defaultdict(list)

    # Check for prefix patterns (GhostPath shares "Ghost" with Ghost)
    for cls in class_names:
        # Find all other classes that share a substantial prefix
        for other in class_names:
            if cls == other:
                continue
            # One name must be a prefix of the other (at least 4 chars)
            shorter, longer = (cls, other) if len(cls) <= len(other) else (other, cls)
            if len(shorter) >= 4 and longer.startswith(shorter):
                groups[shorter].append(longer)

    # Check for suffix patterns (PlayerData, EnemyData share "Data")
    # Common suffixes: Manager, Handler, Service, Data, Config, State, etc.
    common_suffixes = ['Manager', 'Handler', 'Service', 'Data', 'Config', 'State',
                       'Factory', 'Builder', 'Provider', 'Controller', 'Repository']
    for suffix in common_suffixes:
        matching = [c for c in class_names if c.endswith(suffix) and len(c) > len(suffix)]
        if len(matching) >= 2:
            groups[f"*{suffix}"].extend(matching)

    # Deduplicate and filter to groups with 2+ members
    result = {}
    for key, members in groups.items():
        # Deduplicate members first
        unique_members = list(dict.fromkeys(members))  # Preserves order
        # Include the base class if it's a prefix pattern
        if not key.startswith('*') and key in class_names:
            all_members = [key] + [m for m in unique_members if m != key]
        else:
            all_members = unique_members
        if len(all_members) >= 2:
            result[key] = all_members

    return result


def should_exclude(path: Path) -> bool:
    """Check if path should be excluded from scanning."""
    # Check if filename is in exclusion list
    if path.name in EXCLUDE_FILES:
        return True

    parts = path.parts
    for part in parts:
        if part in EXCLUDE_DIRS:
            return True
        # Handle glob patterns like *.egg-info
        for exclude in EXCLUDE_DIRS:
            if '*' in exclude and part.endswith(exclude.replace('*', '')):
                return True
    return False


def count_loc(file_path: Path) -> int:
    """Count lines of code (excluding blank lines and comments)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
    except Exception:
        return 0

    ext = file_path.suffix.lower()
    loc = 0
    in_block_comment = False

    for line in lines:
        stripped = line.strip()

        # Skip blank lines
        if not stripped:
            continue

        # Handle Python/JS/TS comments
        if ext in ('.py',):
            # Python single line comment
            if stripped.startswith('#'):
                continue
            # Python docstrings (simplified)
            if stripped.startswith('"""') or stripped.startswith("'''"):
                if stripped.count('"""') >= 2 or stripped.count("'''") >= 2:
                    continue  # Single line docstring
                in_block_comment = not in_block_comment
                continue
            if in_block_comment:
                continue

        elif ext in ('.ts', '.tsx', '.js', '.jsx', '.css', '.scss'):
            # Single line comment
            if stripped.startswith('//'):
                continue
            # Block comment start
            if stripped.startswith('/*'):
                if '*/' in stripped:
                    continue  # Single line block comment
                in_block_comment = True
                continue
            # Block comment end
            if '*/' in stripped:
                in_block_comment = False
                continue
            if in_block_comment:
                continue

        elif ext == '.md':
            # Count all non-blank lines in markdown
            pass

        loc += 1

    return loc


def get_nesting_depth(rel_path: str) -> int:
    """Get the directory nesting depth from repo root."""
    return len(Path(rel_path).parts) - 1  # -1 because we don't count the file itself


def classify_file_type(rel_path: str, filename: str) -> str:
    """Classify the file type based on path and name patterns."""
    path_lower = rel_path.lower().replace('\\', '/')
    name_lower = filename.lower()

    # Test files
    if '/tests/' in path_lower or name_lower.endswith('.test.ts') or name_lower.endswith('.test.tsx'):
        return 'test'

    # Frontend classifications
    if path_lower.startswith('web/'):
        if '/components/' in path_lower and (name_lower.endswith('.tsx') or name_lower.endswith('.ts')):
            if name_lower.startswith('use'):
                return 'hook'
            return 'component'
        if '/pages/' in path_lower and name_lower.endswith('.tsx'):
            return 'page'
        if '/hooks/' in path_lower or name_lower.startswith('use'):
            return 'hook'
        if '/services/' in path_lower:
            return 'service'
        if '/contexts/' in path_lower:
            return 'context'
        if '/types/' in path_lower or name_lower.endswith('.d.ts'):
            return 'types'
        if '/data/' in path_lower:
            return 'data'
        if '/config/' in path_lower:
            return 'config'
        if name_lower.endswith('.css') or name_lower.endswith('.scss'):
            return 'style'

    # Backend classifications
    if path_lower.startswith('server/'):
        if '/models/' in path_lower:
            return 'model'
        if '/schemas/' in path_lower:
            return 'schema'
        if '/api/' in path_lower:
            return 'api'
        if '/services/' in path_lower:
            return 'service'
        if '/config/' in path_lower:
            return 'config'

    # Core game classifications
    if path_lower.startswith('src/'):
        if '/managers/' in path_lower:
            return 'manager'
        if '/ui/' in path_lower:
            return 'ui'
        if '/entities/' in path_lower:
            return 'entity'
        if '/world/' in path_lower:
            return 'world'
        if '/combat/' in path_lower:
            return 'combat'
        if '/story/' in path_lower:
            return 'story'
        if '/items/' in path_lower:
            return 'items'
        if '/core/' in path_lower:
            return 'core'
        return 'module'

    # Generic classifications by name
    if 'util' in name_lower or 'helper' in name_lower or 'common' in name_lower:
        return 'util'
    if 'constant' in name_lower or 'config' in name_lower:
        return 'constants'
    if name_lower.endswith('.css') or name_lower.endswith('.scss'):
        return 'style'
    if name_lower.endswith('.md'):
        return 'docs'

    return 'other'


def classify_area(rel_path: str) -> str:
    """Classify which area of the codebase a file belongs to."""
    path_lower = rel_path.lower().replace('\\', '/')

    if path_lower.startswith('web/'):
        return 'frontend'
    if path_lower.startswith('server/'):
        return 'backend'
    if path_lower.startswith('src/'):
        return 'core'
    if path_lower.startswith('tests/'):
        return 'tests'
    if path_lower.startswith('docs/') or path_lower.endswith('.md'):
        return 'docs'
    if path_lower.startswith('scripts/'):
        return 'scripts'
    if path_lower.startswith('.claude/'):
        return 'config'

    # Root level config files
    root_configs = {
        'package.json', 'tsconfig.json', 'vite.config.ts', 'docker-compose.yml',
        'requirements.txt', '.gitignore', 'pyproject.toml', 'setup.py'
    }
    if Path(rel_path).name.lower() in root_configs:
        return 'config'

    return 'other'


def get_size_category(loc: int) -> str:
    """Categorize file by lines of code."""
    for category, (min_loc, max_loc) in SIZE_THRESHOLDS.items():
        if min_loc <= loc <= max_loc:
            return category
    return 'xlarge'


@dataclass
class FileAnalysis:
    """Analysis of a file's internal structure."""
    classes: List[str] = field(default_factory=list)
    enums: List[str] = field(default_factory=list)  # Python Enum classes
    base_classes: Dict[str, str] = field(default_factory=dict)  # class -> parent class
    functions: List[str] = field(default_factory=list)
    components: List[str] = field(default_factory=list)
    hooks: List[str] = field(default_factory=list)
    interfaces: List[str] = field(default_factory=list)
    large_functions: List[Tuple[str, int]] = field(default_factory=list)  # (name, line_count)
    decorators: List[str] = field(default_factory=list)
    css_sections: List[str] = field(default_factory=list)
    imports_count: int = 0  # Number of import statements
    imported_by_hint: str = ''  # Hint about what might import this file
    constants: List[str] = field(default_factory=list)  # Module-level constants (UPPER_CASE)


def analyze_python_file(file_path: Path) -> FileAnalysis:
    """Analyze a Python file to extract structure information."""
    analysis = FileAnalysis()
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            lines = content.split('\n')
    except Exception:
        return analysis

    # Count imports
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('import ') or stripped.startswith('from '):
            analysis.imports_count += 1

    # Find module-level constants (UPPER_CASE_NAME = ...)
    constant_pattern = re.compile(r'^([A-Z][A-Z0-9_]+)\s*[=:]')
    for line in lines:
        match = constant_pattern.match(line)
        if match:
            const_name = match.group(1)
            # Skip common non-constant patterns
            if const_name not in ('TYPE_CHECKING', 'T', 'K', 'V'):
                analysis.constants.append(const_name)

    # Find classes with inheritance info
    # Pattern: class Name or class Name(Parent) or class Name(Parent, Mixin)
    class_pattern = re.compile(r'^class\s+(\w+)(?:\s*\(\s*([^)]+)\s*\))?')
    enum_bases = {'Enum', 'IntEnum', 'StrEnum', 'Flag', 'IntFlag', 'auto'}

    for line in lines:
        match = class_pattern.match(line)
        if match:
            class_name = match.group(1)
            parent = match.group(2)

            if parent:
                # Get first parent (primary inheritance)
                first_parent = parent.split(',')[0].strip()
                analysis.base_classes[class_name] = first_parent

                # Check if it's an Enum
                if any(enum_base in parent for enum_base in enum_bases):
                    analysis.enums.append(class_name)
                else:
                    analysis.classes.append(class_name)
            else:
                analysis.classes.append(class_name)

    # Find top-level functions and their sizes
    # Note: A function ends when we hit another def OR a class definition
    func_pattern = re.compile(r'^def\s+(\w+)\s*\(')
    class_boundary = re.compile(r'^class\s+\w+')
    current_func = None
    func_start = 0
    for i, line in enumerate(lines):
        # Check if this line ends the current function (new class or new function)
        is_class_boundary = class_boundary.match(line)
        func_match = func_pattern.match(line)

        if current_func and (is_class_boundary or func_match):
            # Close the previous function
            func_lines = i - func_start
            analysis.functions.append(current_func)
            if func_lines > 50:
                analysis.large_functions.append((current_func, func_lines))
            current_func = None

        if func_match:
            current_func = func_match.group(1)
            func_start = i

    # Don't forget the last function (if file ends with a function)
    if current_func:
        func_lines = len(lines) - func_start
        analysis.functions.append(current_func)
        if func_lines > 50:
            analysis.large_functions.append((current_func, func_lines))

    # Find decorators (for API routes, etc.)
    decorator_pattern = re.compile(r'^@(\w+)')
    for line in lines:
        match = decorator_pattern.match(line.strip())
        if match:
            dec = match.group(1)
            if dec not in analysis.decorators:
                analysis.decorators.append(dec)

    return analysis


def analyze_typescript_file(file_path: Path) -> FileAnalysis:
    """Analyze a TypeScript/React file to extract structure information."""
    analysis = FileAnalysis()
    is_react = file_path.suffix == '.tsx'

    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            lines = content.split('\n')
    except Exception:
        return analysis

    # Find React components (function components and arrow functions)
    # Pattern: export function ComponentName or const ComponentName = (...) =>
    func_component = re.compile(r'(?:export\s+)?function\s+([A-Z]\w+)')
    # Arrow component must have => on same line or shortly after, not just any const
    arrow_component = re.compile(r'(?:export\s+)?const\s+([A-Z]\w+)\s*[=:][^=]*=>')

    for line in lines:
        match = func_component.search(line)
        if match:
            name = match.group(1)
            if not name.startswith('use'):
                analysis.components.append(name)
        match = arrow_component.search(line)
        if match:
            name = match.group(1)
            if not name.startswith('use'):
                analysis.components.append(name)

    # Find hooks (functions starting with 'use') - only for .tsx files
    if is_react:
        hook_pattern = re.compile(r'(?:export\s+)?(?:function|const)\s+(use\w+)\s*[=:(]')
        for line in lines:
            match = hook_pattern.search(line)
            if match:
                hook_name = match.group(1)
                # Exclude useRef, useState etc. calls - only custom hook definitions
                if '= use' not in line or '=>' in line or 'function' in line:
                    analysis.hooks.append(hook_name)

    # Find interfaces and types
    interface_pattern = re.compile(r'(?:export\s+)?(?:interface|type)\s+(\w+)')
    for line in lines:
        match = interface_pattern.search(line)
        if match:
            analysis.interfaces.append(match.group(1))

    # Find large functions - only actual functions, not const arrays/objects
    # Match: function name( or const name = (...) => or const name = function
    # Exclude: const name = [ or const name = { or const name = useRef(
    actual_func_patterns = [
        re.compile(r'^(?:export\s+)?function\s+(\w+)\s*\('),  # function name(
        re.compile(r'^(?:export\s+)?const\s+(\w+)\s*=\s*\([^)]*\)\s*=>'),  # const name = () =>
        re.compile(r'^(?:export\s+)?const\s+(\w+)\s*=\s*function'),  # const name = function
        re.compile(r'^(?:export\s+)?const\s+(\w+)\s*=\s*async\s*\('),  # const name = async (
    ]
    # Patterns that look like functions but aren't (const arrays, objects, refs)
    not_func_patterns = [
        re.compile(r'const\s+\w+\s*[=:][^=]*\['),  # const name = [ or : Type = [
        re.compile(r'const\s+\w+\s*[=:][^=]*\{'),  # const name = { or : Type = {
        re.compile(r'const\s+\w+\s*=\s*use[A-Z]\w*\('),  # const name = useRef(, useState(
        re.compile(r'const\s+\w+\s*=\s*new\s+'),  # const name = new Something(
        re.compile(r'const\s+\w+\s*=\s*\d'),  # const name = 123
        re.compile(r'const\s+\w+\s*=\s*[\'"]'),  # const name = "string"
    ]

    current_func = None
    func_start = 0

    for i, line in enumerate(lines):
        # Skip if line matches a "not function" pattern
        is_not_func = any(p.search(line) for p in not_func_patterns)
        if is_not_func:
            continue

        # Check if line defines an actual function
        for pattern in actual_func_patterns:
            match = pattern.search(line)
            if match:
                # Close previous function
                if current_func:
                    func_lines = i - func_start
                    if func_lines > 80:
                        analysis.large_functions.append((current_func, func_lines))
                current_func = match.group(1)
                func_start = i
                break

    # Handle last function
    if current_func:
        func_lines = len(lines) - func_start
        if func_lines > 80:
            analysis.large_functions.append((current_func, func_lines))

    # Filter out components from large_functions - component size is expected to be large
    # We only want to report helper functions that are too large, not the component itself
    component_names = set(analysis.components)
    analysis.large_functions = [
        (name, lines) for name, lines in analysis.large_functions
        if name not in component_names
    ]

    return analysis


def analyze_css_file(file_path: Path) -> FileAnalysis:
    """Analyze a CSS file to extract major sections."""
    analysis = FileAnalysis()
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
    except Exception:
        return analysis

    # Find top-level class selectors to identify sections
    selector_pattern = re.compile(r'^\.([a-zA-Z][\w-]*)\s*\{', re.MULTILINE)
    selectors = selector_pattern.findall(content)

    # Group by prefix (e.g., .battle-*, .hud-*, .menu-*)
    prefixes = defaultdict(int)
    for sel in selectors:
        parts = sel.split('-')
        if len(parts) > 1:
            prefixes[parts[0]] += 1
        else:
            prefixes[sel] += 1

    # Get the top section prefixes
    sorted_prefixes = sorted(prefixes.items(), key=lambda x: -x[1])
    analysis.css_sections = [p[0] for p in sorted_prefixes[:5] if p[1] >= 3]

    return analysis


def generate_technique(file_path: str, analysis: FileAnalysis, file_type: str, loc: int) -> str:
    """Generate a specific refactoring technique based on file analysis."""
    path = Path(file_path)
    filename = path.stem
    ext = path.suffix

    # Python files
    if ext == '.py':
        techniques = []

        # Handle Enums - they should stay together
        if analysis.enums:
            if len(analysis.enums) >= 5:
                techniques.append(f"Keep {len(analysis.enums)} related Enums together; consider `enums.py` or `types.py`")
            # Don't suggest splitting enum-only files

        # Analyze class hierarchy within file
        parent_counts: Dict[str, List[str]] = defaultdict(list)
        classes_in_file = set(analysis.classes)

        for cls, parent in analysis.base_classes.items():
            if cls not in analysis.enums:  # Skip enums
                parent_counts[parent].append(cls)

        # Find the dominant inheritance pattern
        dominant_parent = None
        dominant_children: List[str] = []
        for parent, children in parent_counts.items():
            if len(children) >= 3:
                dominant_parent = parent
                dominant_children = children
                break

        # Check if parent class is defined in same file
        parent_in_file = dominant_parent in classes_in_file if dominant_parent else False

        # If many subclasses of same parent
        if dominant_parent and len(dominant_children) >= 5:
            if parent_in_file:
                techniques.append(f"{len(dominant_children)} subclasses of `{dominant_parent}` (base class in file) - group by category or keep together")
            else:
                techniques.append(f"{len(dominant_children)} subclasses of `{dominant_parent}` - consider `{dominant_parent.lower()}s.py` or domain-based grouping")

        # Find truly independent classes (no inheritance relationship with others in file)
        independent_classes = []
        schema_base_classes: Set[str] = set()  # Track schema/framework bases used

        for cls in analysis.classes:
            parent = analysis.base_classes.get(cls)

            # Track schema base classes (Pydantic, SQLAlchemy, etc.)
            if parent in SCHEMA_BASES:
                schema_base_classes.add(parent)

            # Independent if: no parent OR parent not in this file AND not a parent of others in file
            is_child_of_local = parent in classes_in_file
            is_parent_of_local = cls in parent_counts and any(c in classes_in_file for c in parent_counts[cls])
            # Also consider schema base as a form of relationship
            has_schema_parent = parent in SCHEMA_BASES

            if not is_child_of_local and not is_parent_of_local and not has_schema_parent:
                independent_classes.append(cls)
            elif is_parent_of_local and not is_child_of_local:
                # This is a base class with children in file - note it separately
                pass

        # Check for semantic naming patterns that indicate related classes
        non_enum_classes = [c for c in analysis.classes if c not in analysis.enums]
        semantic_groups = find_semantic_groups(non_enum_classes)

        # If classes share a schema base, note the pattern
        if schema_base_classes and len(non_enum_classes) >= 2:
            for base in schema_base_classes:
                inheriting = [c for c in analysis.classes
                              if analysis.base_classes.get(c) == base]
                if len(inheriting) >= 2:
                    class_list = ', '.join(f'`{c}`' for c in inheriting[:4])
                    techniques.append(f"{len(inheriting)} `{base}` schemas: {class_list} - keep together as domain models")
                    # Remove from independent since they're related by schema
                    independent_classes = [c for c in independent_classes if c not in inheriting]

        # If classes share semantic naming, note the pattern
        if semantic_groups:
            for pattern, members in list(semantic_groups.items())[:2]:
                member_list = ', '.join(f'`{m}`' for m in members[:3])
                if pattern.startswith('*'):
                    techniques.append(f"Related by `{pattern[1:]}` suffix: {member_list}")
                else:
                    techniques.append(f"Related by `{pattern}` prefix: {member_list}")
                # Remove from independent
                independent_classes = [c for c in independent_classes if c not in members]

        # Detect parent + children pattern (like Entity + Player + Enemy)
        base_with_children = [(p, children) for p, children in parent_counts.items()
                              if p in classes_in_file and len(children) >= 1]

        if base_with_children and not dominant_parent:
            for base, children in base_with_children:
                child_list = ', '.join(f'`{c}`' for c in children[:3])
                techniques.append(f"Base `{base}` with subclasses {child_list} - extract to `{base.lower()}/` package or keep tightly coupled")

        # Check if filename matches a class name (e.g., dungeon.py contains Dungeon class)
        # This indicates the file is organized around that concept, so all classes are related
        filename_matches_class = any(
            filename.lower() == cls.lower() or
            filename.lower() == cls.lower() + 's' or  # pluralized
            cls.lower() in filename.lower()
            for cls in analysis.classes
        )

        # Suggest splits only for truly independent classes (after removing schema and semantic groups)
        # Skip if filename matches a class - indicates domain organization
        if len(independent_classes) >= 3 and not filename_matches_class and not any('subclasses' in t or 'schemas' in t or 'Related' in t for t in techniques):
            class_list = ', '.join(f'`{c}`' for c in independent_classes[:4])
            techniques.append(f"Extract unrelated classes into modules: {class_list}")
        elif len(independent_classes) == 2 and not filename_matches_class and not any('subclasses' in t or 'Base' in t or 'schemas' in t or 'Related' in t for t in techniques):
            techniques.append(f"Split into `{independent_classes[0].lower()}.py` and `{independent_classes[1].lower()}.py`")
        elif len(analysis.classes) == 1 and loc > 400 and not analysis.enums:
            # Single large class - suggest composition or extraction
            class_name = analysis.classes[0]
            techniques.append(f"Break `{class_name}` into smaller classes using composition pattern")

        # If has many functions, group by responsibility
        if len(analysis.functions) > 15:
            # Try to identify groupings by prefix
            prefixes = defaultdict(list)
            for func in analysis.functions:
                parts = func.split('_')
                # Skip common prefixes and private method markers
                if len(parts) > 1 and parts[0] not in ('get', 'set', 'is', 'has', 'do', 'on', '', '_'):
                    # Skip functions starting with _ (private)
                    if not func.startswith('_'):
                        prefixes[parts[0]].append(func)

            if prefixes:
                groups = sorted(prefixes.items(), key=lambda x: -len(x[1]))[:3]
                if groups and len(groups[0][1]) >= 3:
                    group_names = [f"`{g[0]}_*.py`" for g in groups if len(g[1]) >= 2]
                    if group_names:
                        techniques.append(f"Group related functions into: {', '.join(group_names)}")

        # Large functions suggest extraction
        if analysis.large_functions:
            large = analysis.large_functions[:3]
            func_names = ', '.join(f'`{f[0]}` ({f[1]} lines)' for f in large)
            techniques.append(f"Break down large functions: {func_names}")

        # API routes can be split by decorator groups
        if 'router' in analysis.decorators or 'app' in analysis.decorators:
            techniques.append("Split API routes into domain-specific router modules")

        # Constant-heavy files (data files) - skip if enums present (handled above)
        if len(analysis.constants) >= 5 and len(analysis.classes) == 0 and len(analysis.functions) <= 3 and not analysis.enums:
            # Try to group constants by prefix
            prefixes = defaultdict(list)
            for const in analysis.constants:
                parts = const.split('_')
                if len(parts) > 1:
                    prefixes[parts[0]].append(const)
            if prefixes:
                top_prefix = max(prefixes.items(), key=lambda x: len(x[1]))
                if len(top_prefix[1]) >= 3:
                    techniques.append(f"Data file - group by prefix (e.g., `{top_prefix[0].lower()}_data.py`)")
                else:
                    techniques.append("Data file - split by domain or keep together as shared constants")
            else:
                techniques.append("Data file - split by domain or keep together as shared constants")

        if techniques:
            return ' | '.join(techniques)

        # Better fallback based on file context
        if 'api' in filename.lower() or 'route' in filename.lower():
            return "Group related API endpoints into separate router files by domain"
        elif 'service' in filename.lower():
            return "Extract service methods into smaller, focused service classes"
        elif 'util' in filename.lower() or 'helper' in filename.lower():
            return "Group utility functions by domain (e.g., string_utils.py, date_utils.py)"
        elif 'model' in filename.lower():
            return "Split models into separate files, one model per file"
        return f"Review file structure and extract cohesive modules (~200 lines each)"

    # TypeScript/React files
    elif ext in ('.tsx', '.ts'):
        techniques = []

        # Multiple components suggest splitting
        if len(analysis.components) >= 2:
            comp_list = ', '.join(f'`{c}`' for c in analysis.components[:4])
            techniques.append(f"Extract components to separate files: {comp_list}")
        elif len(analysis.components) == 1 and loc > 400:
            # Single large component - suggest extraction of logic
            comp_name = analysis.components[0]
            techniques.append(f"Extract business logic from `{comp_name}` into custom hooks or utilities")

        # Hooks should be in their own files
        if analysis.hooks:
            hook_list = ', '.join(f'`{h}`' for h in analysis.hooks[:3])
            techniques.append(f"Move hooks to `hooks/` directory: {hook_list}")

        # Many interfaces suggest a types file
        if len(analysis.interfaces) >= 5:
            techniques.append(f"Extract {len(analysis.interfaces)} interfaces to `types.ts`")

        # Large functions in components
        if analysis.large_functions:
            large = analysis.large_functions[:2]
            if large:
                func_names = ', '.join(f'`{f[0]}`' for f in large)
                techniques.append(f"Extract logic from large functions: {func_names}")

        # Component-specific suggestions
        if 'Renderer' in filename or '3D' in filename:
            techniques.append("Separate rendering logic, scene setup, and animation into dedicated modules")
        elif 'Form' in filename:
            techniques.append("Extract form validation and submission logic to custom hooks")
        elif 'List' in filename or 'Table' in filename:
            techniques.append("Extract row/item components and pagination logic")

        if techniques:
            return ' | '.join(techniques)

        # Better fallback based on file context
        if 'page' in filename.lower() or 'view' in filename.lower():
            return "Extract page sections into component files; move data fetching to custom hooks"
        elif 'form' in filename.lower():
            return "Extract form fields to components; move validation to a custom useFormValidation hook"
        elif 'list' in filename.lower() or 'table' in filename.lower():
            return "Extract row/item components; move sorting/filtering logic to custom hooks"
        elif 'modal' in filename.lower() or 'dialog' in filename.lower():
            return "Extract modal content to separate component; create reusable Modal wrapper"
        return "Apply container/presenter pattern: extract UI to components, logic to hooks"

    # CSS files
    elif ext in ('.css', '.scss'):
        if analysis.css_sections:
            sections = ', '.join(f'`{s}-*.css`' for s in analysis.css_sections[:3])
            return f"Split by component prefix into: {sections}"
        return "Split into component-scoped CSS modules"

    return "Analyze file structure and extract cohesive modules"


def scan_repository() -> List[FileStats]:
    """Scan all files in the repository."""
    files: List[FileStats] = []

    for root, dirs, filenames in os.walk(REPO_ROOT):
        # Filter out excluded directories
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for filename in filenames:
            file_path = Path(root) / filename
            rel_path = str(file_path.relative_to(REPO_ROOT))

            # Skip excluded paths
            if should_exclude(file_path.relative_to(REPO_ROOT)):
                continue

            # Only include certain extensions
            if file_path.suffix.lower() not in INCLUDE_EXTENSIONS:
                continue

            loc = count_loc(file_path)
            nesting = get_nesting_depth(rel_path)
            file_type = classify_file_type(rel_path, filename)
            area = classify_area(rel_path)
            size_cat = get_size_category(loc)

            files.append(FileStats(
                path=rel_path.replace('\\', '/'),
                loc=loc,
                nesting_depth=nesting,
                file_type=file_type,
                area=area,
                size_category=size_cat
            ))

    # Sort by area, then by path
    files.sort(key=lambda f: (f.area, f.path))
    return files


def is_data_file(path: str) -> bool:
    """Check if file matches data file patterns (intentionally large, don't split)."""
    filename = Path(path).name
    return any(pattern in filename for pattern in DATA_FILE_PATTERNS)


def generate_refactor_suggestions(files: List[FileStats]) -> List[RefactorItem]:
    """Generate refactor suggestions based on file statistics."""
    suggestions: List[RefactorItem] = []
    ref_id = 0

    for f in files:
        # Skip docs and config
        if f.area in ('docs', 'config', 'scripts'):
            continue

        # Skip intentionally large data files
        if is_data_file(f.path):
            continue

        # Analyze file for specific recommendations
        file_path = REPO_ROOT / f.path
        ext = Path(f.path).suffix.lower()

        if ext == '.py':
            analysis = analyze_python_file(file_path)
        elif ext in ('.ts', '.tsx'):
            analysis = analyze_typescript_file(file_path)
        elif ext in ('.css', '.scss'):
            analysis = analyze_css_file(file_path)
        else:
            analysis = FileAnalysis()

        # Generate specific technique
        technique = generate_technique(f.path, analysis, f.file_type, f.loc)

        # Large files (>500 LOC)
        if f.loc > 500:
            ref_id += 1
            # Build specific details based on analysis
            details = [f'Current size: {f.loc} lines']

            # Show enums separately from classes
            if analysis.enums:
                n = len(analysis.enums)
                details.append(f'Contains {n} {pluralize(n, "Enum")}: {", ".join(analysis.enums[:5])}')

            if analysis.classes:
                n = len(analysis.classes)
                # Check for inheritance patterns
                parent_counts: Dict[str, int] = defaultdict(int)
                for cls in analysis.classes:
                    if cls in analysis.base_classes:
                        parent_counts[analysis.base_classes[cls]] += 1

                # Find dominant parent class
                dominant_parent = max(parent_counts.items(), key=lambda x: x[1], default=(None, 0))
                if dominant_parent[1] >= 3:
                    details.append(f'Contains {n} {pluralize(n, "class", "classes")} ({dominant_parent[1]} inherit from {dominant_parent[0]})')
                else:
                    details.append(f'Contains {n} {pluralize(n, "class", "classes")}: {", ".join(analysis.classes[:5])}')

            if analysis.constants and len(analysis.constants) >= 3:
                n = len(analysis.constants)
                const_preview = ', '.join(analysis.constants[:4])
                details.append(f'Contains {n} {pluralize(n, "constant")}: {const_preview}')
            if analysis.functions and len(analysis.functions) > 10:
                n = len(analysis.functions)
                details.append(f'Contains {n} {pluralize(n, "function")}')
            if analysis.components:
                n = len(analysis.components)
                details.append(f'Contains {n} {pluralize(n, "component")}: {", ".join(analysis.components[:5])}')
            if analysis.hooks:
                n = len(analysis.hooks)
                details.append(f'Contains {n} {pluralize(n, "hook")}: {", ".join(analysis.hooks[:5])}')
            if analysis.large_functions:
                large_names = [f'{name} ({lines}L)' for name, lines in analysis.large_functions[:3]]
                details.append(f'Large functions: {", ".join(large_names)}')

            # Add import count for impact assessment
            if analysis.imports_count > 0:
                n = analysis.imports_count
                details.append(f'Has {n} {pluralize(n, "import")} (splitting may require updating {"this" if n == 1 else "these"})')

            details.append('Update imports in dependent files after splitting')

            suggestions.append(RefactorItem(
                id=f'ref-{ref_id:03d}',
                title=f'Split large file: {Path(f.path).name}',
                description=f'File has {f.loc} lines - consider breaking into smaller modules',
                priority='high' if f.loc > 800 else 'medium',
                category=['split-file'],
                effort='large' if f.loc > 800 else 'medium',
                affected_files=[f.path],
                details=details,
                automated_reason=f'LOC > 500 ({f.loc} lines)',
                technique=technique
            ))

        # Large React components (>300 LOC)
        elif f.file_type == 'component' and f.loc > 300:
            ref_id += 1
            details = [f'Current size: {f.loc} lines']
            if analysis.components:
                n = len(analysis.components)
                details.append(f'{n} {pluralize(n, "sub-component")} found: {", ".join(analysis.components[:5])}')
            if analysis.hooks:
                n = len(analysis.hooks)
                details.append(f'{n} {pluralize(n, "hook")} used: {", ".join(analysis.hooks[:5])}')
            if analysis.interfaces:
                n = len(analysis.interfaces)
                details.append(f'{n} {pluralize(n, "type")} defined')
            details.append('Consider custom hooks for complex stateful logic')

            suggestions.append(RefactorItem(
                id=f'ref-{ref_id:03d}',
                title=f'Extract sub-components: {Path(f.path).name}',
                description=f'Component has {f.loc} lines - consider extracting reusable parts',
                priority='medium',
                category=['extract-component'],
                effort='medium',
                affected_files=[f.path],
                details=details,
                automated_reason=f'Component LOC > 300 ({f.loc} lines)',
                technique=technique
            ))

        # Large pages (>400 LOC)
        elif f.file_type == 'page' and f.loc > 400:
            ref_id += 1
            details = [f'Current size: {f.loc} lines']
            if analysis.components:
                n = len(analysis.components)
                details.append(f'{n} inline {pluralize(n, "component")}: {", ".join(analysis.components[:5])}')
            if analysis.hooks:
                n = len(analysis.hooks)
                details.append(f'{n} {pluralize(n, "hook")}: {", ".join(analysis.hooks[:5])}')
            details.append('Consider container/presenter pattern')

            suggestions.append(RefactorItem(
                id=f'ref-{ref_id:03d}',
                title=f'Simplify page: {Path(f.path).name}',
                description=f'Page has {f.loc} lines - extract sections into components',
                priority='medium',
                category=['extract-component', 'reduce-complexity'],
                effort='medium',
                affected_files=[f.path],
                details=details,
                automated_reason=f'Page LOC > 400 ({f.loc} lines)',
                technique=technique
            ))

        # Large CSS files (>800 LOC)
        elif f.file_type == 'style' and f.loc > 800:
            ref_id += 1
            details = [f'Current size: {f.loc} lines']
            if analysis.css_sections:
                details.append(f'Major sections: {", ".join(analysis.css_sections[:5])}')
            details.extend([
                'Consider CSS modules or styled-components',
                'Group styles by component'
            ])

            suggestions.append(RefactorItem(
                id=f'ref-{ref_id:03d}',
                title=f'Split stylesheet: {Path(f.path).name}',
                description=f'Stylesheet has {f.loc} lines - consider component-level splitting',
                priority='low',
                category=['split-file'],
                effort='small',
                affected_files=[f.path],
                details=details,
                automated_reason=f'CSS LOC > 800 ({f.loc} lines)',
                technique=technique
            ))

        # Deep nesting (>6 levels) - increased from 5 to reduce noise
        # Many projects have legitimate 6-level structures (e.g., cutscenes/game_over/scenes/XX_Name/)
        if f.nesting_depth > 6:
            ref_id += 1
            # For deep nesting, suggest a more appropriate location
            path_parts = Path(f.path).parts
            suggested_location = '/'.join(path_parts[:3]) if len(path_parts) > 3 else f.path

            suggestions.append(RefactorItem(
                id=f'ref-{ref_id:03d}',
                title=f'Flatten structure: {f.path}',
                description=f'File is nested {f.nesting_depth} levels deep',
                priority='low',
                category=['consolidate'],
                effort='small',
                affected_files=[f.path],
                details=[
                    f'Current nesting depth: {f.nesting_depth}',
                    f'Consider moving to: {suggested_location}/',
                    'Update import paths in dependent files'
                ],
                automated_reason=f'Nesting depth > 5 ({f.nesting_depth} levels)',
                technique=f'Move to shallower location like `{suggested_location}/`'
            ))

    return suggestions


def generate_typescript(files: List[FileStats], refactors: List[RefactorItem]) -> str:
    """Generate the TypeScript data file content."""
    timestamp = datetime.now().isoformat()

    # File type config
    file_type_config = {
        'component': ('Component', 'box', '#3b82f6'),
        'page': ('Page', 'file', '#8b5cf6'),
        'hook': ('Hook', 'link', '#06b6d4'),
        'service': ('Service', 'server', '#10b981'),
        'context': ('Context', 'share', '#f59e0b'),
        'model': ('Model', 'database', '#ef4444'),
        'schema': ('Schema', 'code', '#ec4899'),
        'api': ('API', 'globe', '#6366f1'),
        'module': ('Module', 'package', '#84cc16'),
        'manager': ('Manager', 'settings', '#f97316'),
        'util': ('Utility', 'tool', '#64748b'),
        'constants': ('Constants', 'hash', '#78716c'),
        'types': ('Types', 'type', '#a855f7'),
        'test': ('Test', 'check', '#22c55e'),
        'style': ('Style', 'palette', '#e879f9'),
        'data': ('Data', 'table', '#0ea5e9'),
        'config': ('Config', 'cog', '#71717a'),
        'docs': ('Docs', 'book', '#94a3b8'),
        'ui': ('UI', 'monitor', '#14b8a6'),
        'entity': ('Entity', 'user', '#f43f5e'),
        'world': ('World', 'map', '#22d3ee'),
        'combat': ('Combat', 'zap', '#ef4444'),
        'story': ('Story', 'scroll', '#a78bfa'),
        'items': ('Items', 'box', '#fbbf24'),
        'core': ('Core', 'cpu', '#6366f1'),
        'other': ('Other', 'file', '#9ca3af'),
    }

    area_config = {
        'frontend': ('Frontend', 'monitor', '#3b82f6'),
        'backend': ('Backend', 'server', '#10b981'),
        'core': ('Core Game', 'gamepad', '#8b5cf6'),
        'tests': ('Tests', 'check-circle', '#22c55e'),
        'docs': ('Documentation', 'book', '#f59e0b'),
        'scripts': ('Scripts', 'terminal', '#6366f1'),
        'config': ('Config', 'cog', '#71717a'),
        'other': ('Other', 'folder', '#9ca3af'),
    }

    # Generate file stats array
    file_stats_lines = []
    for f in files:
        file_stats_lines.append(
            f"  {{ path: '{f.path}', loc: {f.loc}, nestingDepth: {f.nesting_depth}, "
            f"fileType: '{f.file_type}', area: '{f.area}', sizeCategory: '{f.size_category}' }}"
        )

    # Generate refactor items array
    refactor_lines = []
    for r in refactors:
        categories = ', '.join(f"'{c}'" for c in r.category)
        affected = ', '.join(f"'{f}'" for f in r.affected_files)
        # Escape single quotes in details
        escaped_details = [d.replace("'", "\\'") for d in r.details]
        details = ', '.join(f"'{d}'" for d in escaped_details)
        # Escape single quotes in title and description
        escaped_title = r.title.replace("'", "\\'")
        escaped_desc = r.description.replace("'", "\\'")
        # Escape technique field
        escaped_technique = r.technique.replace("'", "\\'").replace('\n', ' ')
        refactor_lines.append(f"""  {{
    id: '{r.id}',
    title: '{escaped_title}',
    description: '{escaped_desc}',
    priority: '{r.priority}',
    status: '{r.status}',
    category: [{categories}],
    effort: '{r.effort}',
    affectedFiles: [{affected}],
    details: [{details}],
    automatedReason: '{r.automated_reason}',
    technique: '{escaped_technique}'
  }}""")

    # Pre-compute joined strings for the template
    file_type_config_lines = '\n'.join(
        f"  {k}: {{ label: '{v[0]}', icon: '{v[1]}', color: '{v[2]}' }},"
        for k, v in file_type_config.items()
    )
    area_config_lines = '\n'.join(
        f"  {k}: {{ label: '{v[0]}', icon: '{v[1]}', color: '{v[2]}' }},"
        for k, v in area_config.items()
    )
    file_stats_joined = ',\n'.join(file_stats_lines)
    refactor_todos_joined = ',\n'.join(refactor_lines)

    # Build the TypeScript content
    ts_content = f'''/**
 * Codebase Health Data
 * Auto-generated by scripts/generate_codebase_health.py
 * Generated: {timestamp}
 *
 * DO NOT EDIT MANUALLY - Run the script to regenerate
 */

// =============================================================================
// Types
// =============================================================================

export type FileType =
  | 'component' | 'page' | 'hook' | 'service' | 'context'
  | 'model' | 'schema' | 'api' | 'module' | 'manager'
  | 'util' | 'constants' | 'types' | 'test' | 'style'
  | 'data' | 'config' | 'docs' | 'ui' | 'entity'
  | 'world' | 'combat' | 'story' | 'items' | 'core' | 'other';

export type Area = 'frontend' | 'backend' | 'core' | 'tests' | 'docs' | 'scripts' | 'config' | 'other';

export type SizeCategory = 'small' | 'medium' | 'large' | 'xlarge';

export type RefactorPriority = 'critical' | 'high' | 'medium' | 'low';

export type RefactorStatus = 'pending' | 'in-progress' | 'completed' | 'deferred';

export type RefactorCategory =
  | 'split-file' | 'reduce-complexity' | 'extract-hook'
  | 'extract-component' | 'consolidate' | 'rename' | 'delete';

export type Effort = 'small' | 'medium' | 'large' | 'epic';

// =============================================================================
// Interfaces
// =============================================================================

export interface FileStats {{
  path: string;
  loc: number;
  nestingDepth: number;
  fileType: FileType;
  area: Area;
  sizeCategory: SizeCategory;
}}

export interface RefactorItem {{
  id: string;
  title: string;
  description: string;
  priority: RefactorPriority;
  status: RefactorStatus;
  category: RefactorCategory[];
  effort: Effort;
  affectedFiles: string[];
  details: string[];
  automatedReason: string;
  technique: string;
}}

// =============================================================================
// Config Objects
// =============================================================================

export const FILE_TYPE_CONFIG: Record<FileType, {{ label: string; icon: string; color: string }}> = {{
{file_type_config_lines}
}};

export const AREA_CONFIG: Record<Area, {{ label: string; icon: string; color: string }}> = {{
{area_config_lines}
}};

export const SIZE_CONFIG: Record<SizeCategory, {{ label: string; color: string; minLoc: number; maxLoc: number }}> = {{
  small: {{ label: 'Small', color: '#22c55e', minLoc: 0, maxLoc: 99 }},
  medium: {{ label: 'Medium', color: '#eab308', minLoc: 100, maxLoc: 299 }},
  large: {{ label: 'Large', color: '#f97316', minLoc: 300, maxLoc: 499 }},
  xlarge: {{ label: 'X-Large', color: '#ef4444', minLoc: 500, maxLoc: Infinity }},
}};

export const REFACTOR_PRIORITY_CONFIG: Record<RefactorPriority, {{ label: string; color: string; order: number }}> = {{
  critical: {{ label: 'Critical', color: '#ef4444', order: 0 }},
  high: {{ label: 'High', color: '#f97316', order: 1 }},
  medium: {{ label: 'Medium', color: '#eab308', order: 2 }},
  low: {{ label: 'Low', color: '#22c55e', order: 3 }},
}};

export const REFACTOR_STATUS_CONFIG: Record<RefactorStatus, {{ label: string; icon: string; color: string }}> = {{
  pending: {{ label: 'Pending', icon: '', color: '#6b7280' }},
  'in-progress': {{ label: 'In Progress', icon: '', color: '#3b82f6' }},
  completed: {{ label: 'Completed', icon: '', color: '#22c55e' }},
  deferred: {{ label: 'Deferred', icon: '', color: '#4b5563' }},
}};

export const REFACTOR_CATEGORY_CONFIG: Record<RefactorCategory, {{ label: string; icon: string }}> = {{
  'split-file': {{ label: 'Split File', icon: '' }},
  'reduce-complexity': {{ label: 'Reduce Complexity', icon: '' }},
  'extract-hook': {{ label: 'Extract Hook', icon: '' }},
  'extract-component': {{ label: 'Extract Component', icon: '' }},
  consolidate: {{ label: 'Consolidate', icon: '' }},
  rename: {{ label: 'Rename', icon: '' }},
  delete: {{ label: 'Delete', icon: '' }},
}};

export const EFFORT_CONFIG: Record<Effort, {{ label: string; dots: number }}> = {{
  small: {{ label: 'Small', dots: 1 }},
  medium: {{ label: 'Medium', dots: 2 }},
  large: {{ label: 'Large', dots: 3 }},
  epic: {{ label: 'Epic', dots: 4 }},
}};

// =============================================================================
// Generated Data
// =============================================================================

export const GENERATED_AT = '{timestamp}';

export const FILE_STATS: FileStats[] = [
{file_stats_joined}
];

export const REFACTOR_TODOS: RefactorItem[] = [
{refactor_todos_joined}
];

// =============================================================================
// Helper Functions
// =============================================================================

export function getFilesByArea(area: Area): FileStats[] {{
  return FILE_STATS.filter((f) => f.area === area);
}}

export function getFilesByType(fileType: FileType): FileStats[] {{
  return FILE_STATS.filter((f) => f.fileType === fileType);
}}

export function getFilesBySize(sizeCategory: SizeCategory): FileStats[] {{
  return FILE_STATS.filter((f) => f.sizeCategory === sizeCategory);
}}

export function getAreaStats(): Record<Area, {{ count: number; totalLoc: number }}> {{
  const stats: Record<string, {{ count: number; totalLoc: number }}> = {{}};
  for (const area of Object.keys(AREA_CONFIG) as Area[]) {{
    const files = getFilesByArea(area);
    stats[area] = {{
      count: files.length,
      totalLoc: files.reduce((sum, f) => sum + f.loc, 0),
    }};
  }}
  return stats as Record<Area, {{ count: number; totalLoc: number }}>;
}}

export function getTypeStats(): Record<FileType, {{ count: number; totalLoc: number }}> {{
  const stats: Record<string, {{ count: number; totalLoc: number }}> = {{}};
  for (const fileType of Object.keys(FILE_TYPE_CONFIG) as FileType[]) {{
    const files = getFilesByType(fileType);
    stats[fileType] = {{
      count: files.length,
      totalLoc: files.reduce((sum, f) => sum + f.loc, 0),
    }};
  }}
  return stats as Record<FileType, {{ count: number; totalLoc: number }}>;
}}

export function getTotalStats(): {{ fileCount: number; totalLoc: number; avgLoc: number }} {{
  const totalLoc = FILE_STATS.reduce((sum, f) => sum + f.loc, 0);
  return {{
    fileCount: FILE_STATS.length,
    totalLoc,
    avgLoc: Math.round(totalLoc / FILE_STATS.length),
  }};
}}

export function getLargestFiles(limit = 10): FileStats[] {{
  return [...FILE_STATS].sort((a, b) => b.loc - a.loc).slice(0, limit);
}}

export function getDeepestFiles(limit = 10): FileStats[] {{
  return [...FILE_STATS].sort((a, b) => b.nestingDepth - a.nestingDepth).slice(0, limit);
}}

export function getRefactorsByPriority(priority: RefactorPriority): RefactorItem[] {{
  return REFACTOR_TODOS.filter((r) => r.priority === priority);
}}

export function getRefactorStats(): {{ total: number; pending: number; completed: number; byPriority: Record<RefactorPriority, number> }} {{
  return {{
    total: REFACTOR_TODOS.length,
    pending: REFACTOR_TODOS.filter((r) => r.status === 'pending').length,
    completed: REFACTOR_TODOS.filter((r) => r.status === 'completed').length,
    byPriority: {{
      critical: REFACTOR_TODOS.filter((r) => r.priority === 'critical').length,
      high: REFACTOR_TODOS.filter((r) => r.priority === 'high').length,
      medium: REFACTOR_TODOS.filter((r) => r.priority === 'medium').length,
      low: REFACTOR_TODOS.filter((r) => r.priority === 'low').length,
    }},
  }};
}}
'''

    return ts_content


def main():
    """Main entry point."""
    print("Scanning repository...")
    files = scan_repository()
    print(f"Found {len(files)} files")

    print("Generating refactor suggestions...")
    refactors = generate_refactor_suggestions(files)
    print(f"Generated {len(refactors)} suggestions")

    print("Generating TypeScript...")
    ts_content = generate_typescript(files, refactors)

    print(f"Writing to {OUTPUT_FILE}...")
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(ts_content)

    # Print summary
    total_loc = sum(f.loc for f in files)
    areas = defaultdict(int)
    for f in files:
        areas[f.area] += 1

    print("\n" + "=" * 50)
    print("CODEBASE HEALTH SUMMARY")
    print("=" * 50)
    print(f"Total files: {len(files)}")
    print(f"Total LOC: {total_loc:,}")
    print(f"Average LOC: {total_loc // len(files) if files else 0}")
    print("\nFiles by area:")
    for area, count in sorted(areas.items()):
        print(f"  {area}: {count}")
    print(f"\nRefactor suggestions: {len(refactors)}")
    print(f"\nOutput: {OUTPUT_FILE}")
    print("=" * 50)


if __name__ == '__main__':
    main()
